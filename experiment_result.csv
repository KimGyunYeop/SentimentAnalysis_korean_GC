모델이름,성능,step,ckpt,consoloutput,설명,날짜,참조,비고,사용 모델 명,서버
KOELECTRA_FOR_SEQ_CLS,0.926492469,20000,koelectra-base-nsmc-ckpt,KOELECTRA_CLS.out,electraforsequenceclassification finetuning model을 사용하는 classifcation,07월 15일,,logging step을 10000으로하여 불확실,transformers.ElectraForSequenceClassification,217
koELECTRA+LSTM,0.927562968,4000,koeletra_lstm,KOELETRA_LSTM.out,koELECTRA embedding에 one direcitonal LSTM후 마지막 hidden state의 classification,07월 16일,,,model.LSTM,217
KoELECTRA + LSTM + ATT,0.92809485,24000,koeletra-lstm-att-cls,output_LSTM_ATT.out,KOELECTRA embedding에 one direcitonal LSTM후 Attention layer의 classification,07월 22일,,,model.LSTM_ATT,217
KoELECTRA + LSTM + DOTATT,0.927031085,20000,koeletra-lstm-att-cls,x,KOELECTRA embedding에 one direcitonal LSTM후 마지막 hidden과 seq hidden의 DotProduct Attention layer의 classification,07월 22일,,,model.LSTM_ATT_DOT,218
,,,,,,,,,,
KoBERT + LSTM + ATT,0.7551,all same,kobert-lstm-att-cls,KoBERT_LSTM_ATT.out,KoBERT embedding에 one direcitonal LSTM후 Attention layer의 classification,07월 22일,,다시해봐야할듯 정확도도 너무 낮고 acc가 모두 같음. 학습이 되는지 의심이 됨.,model.LSTM_ATT,217
koELECTRA + LSTM + ATTv2,0.9259,48000,koelectra-lstm-att-v2-cls,KoELECTRA_LSTM_ATT_V2.out,KoELECTRA embedding에 one direcitonal LSTM후 v2의 Attention layer의 classification,07월 22일,,batch_size = 128,model.LSTM_ATT_v2,218
KoBERT + LSTM + DOTATT,0.9185,32000,kobert-lstm-dotatt-cls,KoBERT_LSTM_DOCATT.out,KoBERT embedding에 one direcitonal LSTM후 DotProduct Attention layer의 classification,07월 22일,,,model.LSTM_ATT_DOT,218
koBERT + LSTM + ATTv2,,,kobert-lstm-att-v2-cls,KoBERT_LSTM_ATT_V2.out,KoBERT embedding에 one direcitonal LSTM후 v2의 Attention layer의 classification,07월 23일,,batch_size = 128,model.LSTM_ATT_v2,218
,,,,,,,,,,
kosac+ATTdot,0.926492,,LSTM_ATT_DOT_KOSAC,kosac_att_dot.out,SNU lexicon 접목한 ATT dot model,08월 06일,,,model.LSTM_ATT_DOT_KOSAC,217
KOSAC_LSTM_ATT_v2,0.925024,,LSTM_ATT_v2_KOSAC,kosac_att_v2,SNU lexicon 접목한 ATT model,08월 06일,,,model.LSTM_ATT_v2_KOSAC,218
KOSAC_LSTM_ATT,0.92627702,,LSTM_ATT_KOSAC,kosac_att,SNU lexicon 접목한 ATT model,08월 06일,,,model.LSTM_ATT_KOSAC,218
LSTM_KOSAC,0.92549603,,LSTM_KOSAC,kosac,SNU lexicon 접목한 koelectra + LSTM,08월 06일,,last를 포함하여 연산 재실험 필요,model.LSTM_KOSAC,225
,,,,,,,,,,
KNU_BASE (ELECTRA),0.9272,6000,KNU_BASE,KNU_BASE.txt,KNU lexicon 을 electra이후에 접목한 모델,08월 20일,,,model.LSTM_ATT_DOT_KOSAC,218
LSTM_KOSAC,0.9266,54000,LSTM_ATT_DOT_KOSAC,LSTM_ATT_DOT_KNU.txt,KNU lexicon 을 electra이후에 접목한 후 dot att를 한 모델,08월 19일,,,model.KNU_BASE,218
,,,,,,,,,,
koELECTRABASE,0.925839398,,LSTM_KOSAC,kosac,SNU lexicon을 electra이후에 접목한 koelectra + LSTM,08월 07일,,,model.LSTM_KOSAC,225
koELECTRABASE_COS,,,,,,,,,,
koELECTRABASE_COS2,0.9273,26000,BASEELECTRA,BASEELECTRA.out,koelectra cls에서 classification,08월 20일,,256 batch,model/BASEELECTRA,217
koELECTRABASE_COS,0.9241,22000,BASEELECTRA_COS,BASEELECTRA_COS.out,koelectra cls에서 classification에 cossim loss를 추가한 모델,08월 20일,,256 배치 margin모름,model/BASEELECTRA_COS,217
koELECTRABASE_COS2,0.9271,2000,BASEELECTRA_COS2,cos2.out,koelectra cls에서 classification에 cossim loss와 star cossim loss를 추가한 모델 ,08월 20일,,400 batch margin -0.5,model/BASEELECTRA_COS2,217
koELECTRABASE_COS,,,BASEELECTRA_COS,cos1.out,koelectra cls에서 classification에 cossim loss를 추가한 모델,08월 20일,,256 배치 margin 1,model/BASEELECTRA_COS,218
,0.9257,2000,BASEELECTRA_COS2,ELECTRABASE_cos2.out,koelectra cls에서 classification에 cossim loss와 star cossim loss를 추가한 모델 ,08월 20일,,256 배치 margin모름,model/BASEELECTRA_COS2,218
,,,BASEELECTRA_COS,/ELECTRABASE_cos.out,koelectra cls에서 classification에 cossim loss를 추가한 모델,08월 20일,,400 batch margin -0.5,model/BASEELECTRA_COS,217
BASEELECTRA_COS_NEG,0.926263558,2000,BASE_COS_NEG,0820_cos_neg.txt,BASEELECTRA_COS에 negative sampling 추가한 모델,08월 30일,,170 배치 -> 배치 늘려서 다시 실험해보자,model/BASEELECTRA_COS_NEG,218
BASEELECTRA_COS2_NEG,0.925226723,2000,BASE_COS_NEG_2,0830_cos_neg.txt,BASEELECTRA_COS2에 negative sampling 추가한 모델,08월 30일,,170 배치,model/BASEELECTRA_COS2_NEG,218
BASEELECTRA_COS2_STAR_NEG,0.926371281,2000,BASE_COS_STAR_NEG,0831_cos_star_neg_2.txt,BASEELECTRA_COS2_NEG에 star벡터의 negative sampling 추가한 모델,08월 31일,,"170 배치, 최적화가 너무 빠름",model/BASEELECTRA_COS2_STAR_NEG,218
BASE_KNU_ALL,,2000,BASE_KNU_ALL,0831_knu_all.txt,knu_base에서 lexicon을 knu_origin+naver_dc로 바꾼 것,08월 31일,,170 배치,model/BASEELECTRA_COS2_STAR_NEG,218
