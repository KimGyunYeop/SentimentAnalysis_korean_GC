모델이름,성능,step,ckpt,consoloutput,설명,날짜,참조,비고,사용 모델 명,서버
KOELECTRA_FOR_SEQ_CLS,0.926492469,20000,koelectra-base-nsmc-ckpt,KOELECTRA_CLS.out,electraforsequenceclassification finetuning model을 사용하는 classifcation,07월 15일,,logging step을 10000으로하여 불확실,transformers.ElectraForSequenceClassification,217
koELECTRA+LSTM,0.927562968,4000,koeletra_lstm,KOELETRA_LSTM.out,koELECTRA embedding에 one direcitonal LSTM후 마지막 hidden state의 classification,07월 16일,,,model.LSTM,217
KoELECTRA + LSTM + ATT,0.92809485,24000,koeletra-lstm-att-cls,output_LSTM_ATT.out,KOELECTRA embedding에 one direcitonal LSTM후 Attention layer의 classification,07월 22일,,,model.LSTM_ATT,217
KoELECTRA + LSTM + DOTATT,0.927031085,20000,koeletra-lstm-att-cls,x,KOELECTRA embedding에 one direcitonal LSTM후 마지막 hidden과 seq hidden의 DotProduct Attention layer의 classification,07월 22일,,,model.LSTM_ATT_DOT,218
,,,,,,,,,,
KoBERT + LSTM + ATT,0.7551,all same,kobert-lstm-att-cls,KoBERT_LSTM_ATT.out,KoBERT embedding에 one direcitonal LSTM후 Attention layer의 classification,07월 22일,,다시해봐야할듯 정확도도 너무 낮고 acc가 모두 같음. 학습이 되는지 의심이 됨.,model.LSTM_ATT,217
koELECTRA + LSTM + ATTv2,0.9259,48000,koelectra-lstm-att-v2-cls,KoELECTRA_LSTM_ATT_V2.out,KoELECTRA embedding에 one direcitonal LSTM후 v2의 Attention layer의 classification,07월 22일,,batch_size = 128,model.LSTM_ATT_v2,218
KoBERT + LSTM + DOTATT,0.9185,32000,kobert-lstm-dotatt-cls,KoBERT_LSTM_DOCATT.out,KoBERT embedding에 one direcitonal LSTM후 DotProduct Attention layer의 classification,07월 22일,,,model.LSTM_ATT_DOT,218
koBERT + LSTM + ATTv2,,,kobert-lstm-att-v2-cls,KoBERT_LSTM_ATT_V2.out,KoBERT embedding에 one direcitonal LSTM후 v2의 Attention layer의 classification,07월 23일,,batch_size = 128,model.LSTM_ATT_v2,218
,,,,,,,,,,
kosac+ATTdot,0.926492,,LSTM_ATT_DOT_KOSAC,kosac_att_dot.out,SNU lexicon 접목한 ATT dot model,08월 06일,,,model.LSTM_ATT_DOT_KOSAC,217
KOSAC_LSTM_ATT_v2,0.925024,,LSTM_ATT_v2_KOSAC,kosac_att_v2,SNU lexicon 접목한 ATT model,08월 06일,,,model.LSTM_ATT_v2_KOSAC,218
KOSAC_LSTM_ATT,0.92627702,,LSTM_ATT_KOSAC,kosac_att,SNU lexicon 접목한 ATT model,08월 06일,,,model.LSTM_ATT_KOSAC,218
LSTM_KOSAC,0.92549603,,LSTM_KOSAC,kosac,SNU lexicon 접목한 koelectra + LSTM,08월 06일,,last를 포함하여 연산 재실험 필요,model.LSTM_KOSAC,225
,,,,,,,,,,
kosac+ATTdot,0.925502764,,LSTM_ATT_DOT_KOSAC,kosac_att_dot.out,SNU lexicon 을 electra이후에 접목한 ATT dot model,08월 07일,,,model.LSTM_ATT_DOT_KOSAC,217
KOSAC_LSTM_ATT_v2,0.92519305,,LSTM_ATT_v2_KOSAC,kosac_att_v2,SNU lexicon을 electra이후에 접목한 ATT model,08월 07일,,,model.LSTM_ATT_v2_KOSAC,218
KOSAC_LSTM_ATT,0.92464097,,LSTM_ATT_KOSAC,kosac_att,SNU lexicon을 electra이후에 접목한 ATT model,08월 07일,,,model.LSTM_ATT_KOSAC,218
LSTM_KOSAC,0.925839398,,LSTM_KOSAC,kosac,SNU lexicon을 electra이후에 접목한 koelectra + LSTM,08월 07일,,,model.LSTM_KOSAC,225
,,,,,,,,,,
koELECTRABASE,0.9273,26000,BASEELECTRA,BASEELECTRA.out,koelectra cls에서 classification,08월 20일,,256 batch,model/BASEELECTRA,217
koELECTRABASE_COS,0.9241,22000,BASEELECTRA_COS,BASEELECTRA_COS.out,koelectra cls에서 classification에 cossim loss를 추가한 모델,08월 20일,,256 배치 margin모름,model/BASEELECTRA_COS,217
koELECTRABASE_COS2,0.9271,2000,BASEELECTRA_COS2,cos2.out,koelectra cls에서 classification에 cossim loss와 star cossim loss를 추가한 모델 ,08월 20일,,400 batch margin -0.5,model/BASEELECTRA_COS2,217
koELECTRABASE_COS,,,BASEELECTRA_COS,cos1.out,koelectra cls에서 classification에 cossim loss를 추가한 모델,08월 20일,,256 배치 margin 1,model/BASEELECTRA_COS,218
koELECTRABASE_COS2,0.9257,2000,BASEELECTRA_COS2,ELECTRABASE_cos2.out,koelectra cls에서 classification에 cossim loss와 star cossim loss를 추가한 모델 ,08월 20일,,256 배치 margin모름,model/BASEELECTRA_COS2,218
koELECTRABASE_COS,,,BASEELECTRA_COS,/ELECTRABASE_cos.out,koelectra cls에서 classification에 cossim loss를 추가한 모델,08월 20일,,400 batch margin -0.5,model/BASEELECTRA_COS,217
